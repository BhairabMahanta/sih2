<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ML Inference</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.22.0/dist/ort.min.js"></script>
</head>
<body>
    <script>
        let session = null;

        // Initialize ONNX model
        async function initModel() {
            try {
                console.log('Loading ONNX model...');
                
                // You'll need to serve your model file
                const response = await fetch('./model.onnx');
                const modelBuffer = await response.arrayBuffer();
                
                session = await ort.InferenceSession.create(modelBuffer);
                console.log('✅ Model loaded successfully');
                
                // Notify React Native that model is ready
                window.ReactNativeWebView?.postMessage(JSON.stringify({
                    type: 'model_ready',
                    status: 'success'
                }));
                
            } catch (error) {
                console.error('❌ Model loading failed:', error);
                window.ReactNativeWebView?.postMessage(JSON.stringify({
                    type: 'model_ready',
                    status: 'error',
                    message: error.message
                }));
            }
        }

        // Run inference
        async function runInference(imageDataUrl) {
            try {
                if (!session) {
                    throw new Error('Model not loaded');
                }

                // Convert base64 image to tensor
                const tensor = await preprocessImage(imageDataUrl);
                
                // Run inference
                const results = await session.run({ 'input': tensor });
                const outputTensor = results['output'];
                const predictions = outputTensor.data;
                
                // Find best prediction (your model outputs [1,3])
                let maxIndex = 0;
                let maxValue = predictions[0];
                
                for (let i = 1; i < 3; i++) {
                    if (predictions[i] > maxValue) {
                        maxValue = predictions[i];
                        maxIndex = i;
                    }
                }

                // Send results back to React Native
                window.ReactNativeWebView?.postMessage(JSON.stringify({
                    type: 'inference_result',
                    class: maxIndex,
                    confidence: maxValue
                }));
                
            } catch (error) {
                console.error('Inference error:', error);
                window.ReactNativeWebView?.postMessage(JSON.stringify({
                    type: 'inference_error',
                    message: error.message
                }));
            }
        }

        // Preprocess image for your model
        async function preprocessImage(imageDataUrl) {
            return new Promise((resolve) => {
                const canvas = document.createElement('canvas');
                const ctx = canvas.getContext('2d');
                const img = new Image();
                
                img.onload = () => {
                    // Resize to 224x224
                    canvas.width = 224;
                    canvas.height = 224;
                    ctx.drawImage(img, 0, 0, 224, 224);
                    
                    // Get image data
                    const imageData = ctx.getImageData(0, 0, 224, 224);
                    const data = imageData.data;
                    
                    // Convert to Float32Array [1, 3, 224, 224]
                    const float32Data = new Float32Array(3 * 224 * 224);
                    
                    for (let i = 0; i < 224 * 224; i++) {
                        const pixelIndex = i * 4;
                        float32Data[i] = data[pixelIndex] / 255.0;                    // R
                        float32Data[224 * 224 + i] = data[pixelIndex + 1] / 255.0;    // G
                        float32Data[224 * 224 * 2 + i] = data[pixelIndex + 2] / 255.0; // B
                    }
                    
                    const tensor = new ort.Tensor('float32', float32Data, [1, 3, 224, 224]);
                    resolve(tensor);
                };
                
                img.src = imageDataUrl;
            });
        }

        // Listen for messages from React Native
        window.addEventListener('message', (event) => {
            const data = JSON.parse(event.data);
            
            if (data.type === 'run_inference') {
                runInference(data.imageDataUrl);
            }
        });

        // Initialize when page loads
        window.addEventListener('load', initModel);
    </script>
</body>
</html>
